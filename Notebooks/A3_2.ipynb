{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Student Name:** Mahebobe Fayyaz Savanur\n",
        "\n",
        "**Student ID:** 47969725"
      ],
      "metadata": {
        "id": "RooTr0-8prsL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9pWsPAt0dCh"
      },
      "source": [
        "# Assignment 3 Part 2 - Find complex answers to medical questions\n",
        "\n",
        "*Submission deadline: Friday 24 May 2024, 11:55pm.*\n",
        "\n",
        "*Assessment marks: 15 marks (15% of the total unit assessment)*\n",
        "\n",
        "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted. The submission time for all uploaded assessments is 11:55 pm. A 1-hour grace period will be provided to students who experience a technical concern. For any late submission of time-sensitive tasks, such as scheduled tests/exams, performance assessments/presentations, and/or scheduled practical assessments/labs, please apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration).\n",
        "\n",
        "Note that the work submitted should be your own work. You are allowed to use AI-based code generators to help you understand the problem and possible solutions, but you are not allowed to use the code generated by these tools (see below).\n",
        "\n",
        "You are allowed to base your code on the code presented in the unit lectures and lecture notebooks.\n",
        "\n",
        "**A note on the use of AI generators**: In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University.\n",
        "\n",
        "Artificial Intelligence Tools and Academic Integrity in FSE - https://bit.ly/3uxgQP4\n",
        "If you choose to use these tools, make the following explicit in your Jupyter notebook, under a section with heading \"Use of AI generators in this assignment\" :\n",
        "\n",
        "* What part of your code is based on the output of such tools,\n",
        "* What tools you used,\n",
        "* What prompts you used to generate the code or text, and\n",
        "* What modifications you made on the generated code or text.\n",
        "  \n",
        "This will help us assess your work fairly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl4JK9mROxTw"
      },
      "source": [
        "# Overall Task Review\n",
        "\n",
        "In assignment 3 you will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question. Assignment 3 is divided into two parts. Part 1 will help you get familar with the data, and Part 2 requires you to implement deep neural networks.\n",
        "\n",
        "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZxjY9AZOxTx"
      },
      "source": [
        "# Data Review\n",
        "\n",
        "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysxatcUqOxTy"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "o1dMkKNqOxTz",
        "outputId": "bd27d086-3ce7-454c-d699-1023306932ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-41faa375-717e-495b-9701-8735211d66cf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-41faa375-717e-495b-9701-8735211d66cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving bioasq10b_labelled.csv to bioasq10b_labelled (1).csv\n",
            "Saving dev_test.csv to dev_test (1).csv\n",
            "Saving test.csv to test (1).csv\n",
            "Saving training.csv to training (1).csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   qid  sentid                                           question  \\\n",
              "0    0       0  Is Hirschsprung disease a mendelian or a multi...   \n",
              "1    0       1  Is Hirschsprung disease a mendelian or a multi...   \n",
              "2    0       2  Is Hirschsprung disease a mendelian or a multi...   \n",
              "3    0       3  Is Hirschsprung disease a mendelian or a multi...   \n",
              "4    0       4  Is Hirschsprung disease a mendelian or a multi...   \n",
              "\n",
              "                                       sentence text  label  \n",
              "0  Hirschsprung disease (HSCR) is a multifactoria...      0  \n",
              "1  In this study, we review the identification of...      1  \n",
              "2  The majority of the identified genes are relat...      1  \n",
              "3  The non-Mendelian inheritance of sporadic non-...      1  \n",
              "4                  Coding sequence mutations in e.g.      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09acd4b2-6c0b-4734-9ea9-56d7c7425903\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>sentid</th>\n",
              "      <th>question</th>\n",
              "      <th>sentence text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
              "      <td>Hirschsprung disease (HSCR) is a multifactoria...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
              "      <td>In this study, we review the identification of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
              "      <td>The majority of the identified genes are relat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
              "      <td>The non-Mendelian inheritance of sporadic non-...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>Is Hirschsprung disease a mendelian or a multi...</td>\n",
              "      <td>Coding sequence mutations in e.g.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09acd4b2-6c0b-4734-9ea9-56d7c7425903')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09acd4b2-6c0b-4734-9ea9-56d7c7425903 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09acd4b2-6c0b-4734-9ea9-56d7c7425903');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5796961b-ff6e-4d3f-8b66-46af419249e8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5796961b-ff6e-4d3f-8b66-46af419249e8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5796961b-ff6e-4d3f-8b66-46af419249e8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 64112,\n  \"fields\": [\n    {\n      \"column\": \"qid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1221,\n        \"min\": 0,\n        \"max\": 4233,\n        \"num_unique_values\": 4234,\n        \"samples\": [\n          1173,\n          2236,\n          1578\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 0,\n        \"max\": 145,\n        \"num_unique_values\": 146,\n        \"samples\": [\n          45,\n          97,\n          27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4234,\n        \"samples\": [\n          \"Do Parkinson's disease patients experience stridor?\",\n          \"Is sonidegib effective for basal cell carcinoma?\",\n          \"Which are the main histone modifications associated with enhancers?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 58190,\n        \"samples\": [\n          \"Some of those characteristics could explain the high rates of disability and worse prognosis observed in women with RA in LAC\",\n          \"We employed a discovery-based proteomic approach in subcellular fractions of hippocampal tissue from chronic intermittent alcohol (CIE)-exposed C57Bl/6J mice to gain insight into alcohol-induced changes in GluN2B signaling complexes.\",\n          \"Here we report that enhancer RNAs (eRNAs) identified by global nuclear run-on sequencing are extensively transcribed within super enhancers and are dynamically regulated in response to cellular signaling.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpLwO8nTOxT0"
      },
      "source": [
        "The columns of the CSV file are:\n",
        "\n",
        "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
        "* `sentid`: an ID for a sentence.\n",
        "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
        "* `sentence text`: The text of the sentence.\n",
        "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTRWXY5LOxT1"
      },
      "source": [
        "# Now Let's get started for the Part 2 tasks\n",
        "\n",
        "Use the provided files `training.csv`, `dev_test.csv`, and `test.csv` in the data.zip file for all the tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VTTgRnN0dC4"
      },
      "source": [
        "# Task 1 (5 marks): Simple Siamese NN\n",
        "\n",
        "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
        "\n",
        "1. An input layer that will accept the tf.idf of triplet data. The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
        "2. 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
        "3. Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
        "4. Implement a function that prepares raw data in csv files into triplets. Note that it is important to keep the similar number of positive pairs and negative pairs. For example, if a question has 10 anwsers, then we at most can have 10 positive pairs and it is good to associate this question with 10~20 negative sentences.\n",
        "\n",
        "\n",
        "Train the model with the training data and use the `dev_test` set to determine a good size of the hidden layer.\n",
        "\n",
        "With the model that you have trained, implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
        "\n",
        "```{python}\n",
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "   \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
        "      The input questionids is a list of question ids.\n",
        "      The output is a list of lists of sentence ids\n",
        "   \"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes input to the model correctly.\n",
        "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Dense,LSTM, Layer,Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "i1msCDyuWUpG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Load and Prepare Data\n",
        "# Load data\n",
        "train_data = pd.read_csv('training.csv').head(500)\n",
        "dev_test_data = pd.read_csv('dev_test.csv').head(300)\n",
        "test_data = pd.read_csv('test.csv').head(300)\n",
        "\n",
        "# Function to prepare triplets\n",
        "def prepare_triplets(data):\n",
        "    triplets = []\n",
        "    for qid, group in data.groupby('qid'):\n",
        "        question = group['question'].values[0]\n",
        "        positive_answers = group[group['label'] == 1][['sentid', 'sentence text']].values.tolist()\n",
        "        negative_answers = group[group['label'] == 0][['sentid', 'sentence text']].values.tolist()\n",
        "        for pos in positive_answers:\n",
        "            for neg in negative_answers:\n",
        "                triplets.append((qid, question, pos[0], pos[1], neg[0], neg[1]))\n",
        "    return triplets\n",
        "\n",
        "# Prepare triplets for each dataset\n",
        "train_triplets = prepare_triplets(train_data)\n",
        "dev_test_triplets = prepare_triplets(dev_test_data)\n",
        "test_triplets = prepare_triplets(test_data)\n"
      ],
      "metadata": {
        "id": "UuyvXwVZcB-R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_triplets(triplets, vectorizer):\n",
        "    qids = []\n",
        "    pos_ids = []\n",
        "    neg_ids = []\n",
        "    anchors = []\n",
        "    positives = []\n",
        "    negatives = []\n",
        "    for qid, anchor, pos_id, positive, neg_id, negative in triplets:\n",
        "        qids.append(qid)\n",
        "        pos_ids.append(pos_id)\n",
        "        neg_ids.append(neg_id)\n",
        "        anchors.append(vectorizer.transform([anchor]).toarray().flatten())\n",
        "        positives.append(vectorizer.transform([positive]).toarray().flatten())\n",
        "        negatives.append(vectorizer.transform([negative]).toarray().flatten())\n",
        "    return np.array(qids), np.array(pos_ids), np.array(neg_ids), np.array(anchors), np.array(positives), np.array(negatives)\n",
        "\n",
        "# Combine all texts to fit the vectorizer\n",
        "all_sentences = pd.concat([train_data['sentence text'], dev_test_data['sentence text'], test_data['sentence text']])\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(all_sentences)\n",
        "\n",
        "# Vectorize each dataset\n",
        "train_qids, train_pos_ids, train_neg_ids, train_anchors, train_positives, train_negatives = vectorize_triplets(train_triplets, vectorizer)\n",
        "dev_test_qids, dev_test_pos_ids, dev_test_neg_ids, dev_test_anchors, dev_test_positives, dev_test_negatives = vectorize_triplets(dev_test_triplets, vectorizer)\n",
        "test_qids, test_pos_ids, test_neg_ids, test_anchors, test_positives, test_negatives = vectorize_triplets(test_triplets, vectorizer)\n",
        "\n",
        "# Ensure the input dimensions are consistent\n",
        "input_dim = train_anchors.shape[1]\n"
      ],
      "metadata": {
        "id": "1Emucb5ecCI5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Build the Siamese Network\n",
        "def create_siamese_network(input_dim, layer_sizes):\n",
        "    input = Input(shape=(input_dim,))\n",
        "    x = input\n",
        "    for size in layer_sizes:\n",
        "        x = Dense(size, activation='relu')(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "class DistanceLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor, positive, negative = inputs\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), axis=1, keepdims=True)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), axis=1, keepdims=True)\n",
        "        return tf.concat([ap_distance, an_distance], axis=1)\n",
        "\n",
        "def triplet_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    ap_distance = y_pred[:, 0]  # Distance between anchor and positive\n",
        "    an_distance = y_pred[:, 1]  # Distance between anchor and negative\n",
        "    return tf.reduce_mean(tf.maximum(ap_distance - an_distance + margin, 0))\n"
      ],
      "metadata": {
        "id": "yt29kb42cCRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_configs = [\n",
        "    [128, 64, 32],\n",
        "    [256, 128, 64],\n",
        "    [64, 32, 16],\n",
        "    [128, 128, 128],\n",
        "]\n",
        "\n",
        "best_config = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "def evaluate_model(model, questions, positive_answers, negative_answers):\n",
        "    dummy_labels = np.zeros((questions.shape[0], 1))\n",
        "    return model.evaluate([questions, positive_answers, negative_answers], dummy_labels, verbose=0)\n",
        "\n",
        "for config in hidden_layer_configs:\n",
        "    print(f\"Evaluating model with hidden layer sizes: {config}\")\n",
        "\n",
        "    # Create the base network\n",
        "    shared_network = create_siamese_network(input_dim, config)\n",
        "\n",
        "    # Define inputs\n",
        "    anchor_input = Input(shape=(input_dim,), name='anchor')\n",
        "    positive_input = Input(shape=(input_dim,), name='positive')\n",
        "    negative_input = Input(shape=(input_dim,), name='negative')\n",
        "\n",
        "    # Process each input through the shared network\n",
        "    anchor_output = shared_network(anchor_input)\n",
        "    positive_output = shared_network(positive_input)\n",
        "    negative_output = shared_network(negative_input)\n",
        "\n",
        "    # Compute the distances\n",
        "    distances = DistanceLayer()([anchor_output, positive_output, negative_output])\n",
        "\n",
        "    # Define the model\n",
        "    siamese_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)\n",
        "\n",
        "    # Compile the model\n",
        "    siamese_model.compile(optimizer='adam', loss=triplet_loss)\n",
        "\n",
        "    # Define dummy labels for loss calculation\n",
        "    dummy_labels = np.zeros((train_anchor.shape[0], 1))\n",
        "\n",
        "    # Train the model\n",
        "    siamese_model.fit([train_anchor, train_positive, train_negative],\n",
        "                      dummy_labels,\n",
        "                      epochs=5,\n",
        "                      batch_size=32,\n",
        "                      validation_data=([dev_test_anchor, dev_test_positive, dev_test_negative], np.zeros((dev_test_anchor.shape[0], 1))),\n",
        "                      verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss = evaluate_model(siamese_model, dev_test_anchor, dev_test_positive, dev_test_negative)\n",
        "    print(f\"Validation loss: {loss}\")\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        best_config = config\n",
        "\n",
        "print(f\"Best hidden layer configuration: {best_config} with loss: {best_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koC7uXRmcChM",
        "outputId": "3abfe8b2-c9cb-42c1-d20c-971c0339004e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 2s 23ms/step - loss: 0.4820 - val_loss: 0.9498\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.0060 - val_loss: 0.9696\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.0029 - val_loss: 0.9835\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.0020 - val_loss: 0.9855\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.0018 - val_loss: 0.9869\n",
            "Validation loss: 0.9869396090507507\n",
            "Evaluating model with hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 3s 33ms/step - loss: 0.3553 - val_loss: 1.1737\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.0048 - val_loss: 1.1994\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.0035 - val_loss: 1.3806\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 3s 49ms/step - loss: 0.0032 - val_loss: 1.3898\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 3s 61ms/step - loss: 0.0025 - val_loss: 1.3728\n",
            "Validation loss: 1.3727999925613403\n",
            "Evaluating model with hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 5s 43ms/step - loss: 0.6247 - val_loss: 1.0432\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 0.0104 - val_loss: 1.0849\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 2s 32ms/step - loss: 0.0036 - val_loss: 1.0831\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.0023 - val_loss: 1.0839\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.0022 - val_loss: 1.0813\n",
            "Validation loss: 1.0812731981277466\n",
            "Evaluating model with hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 4s 31ms/step - loss: 0.4099 - val_loss: 1.1491\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.0047 - val_loss: 1.2115\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 1s 27ms/step - loss: 0.0022 - val_loss: 1.2183\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 1s 26ms/step - loss: 0.0019 - val_loss: 1.2430\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 2s 41ms/step - loss: 0.0038 - val_loss: 1.2458\n",
            "Validation loss: 1.2458255290985107\n",
            "Best hidden layer configuration: [128, 64, 32] with loss: 0.9869396090507507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training the final model with best hidden layer configuration: {best_config}\")\n",
        "\n",
        "# Create the base network with the best configuration\n",
        "shared_network = create_siamese_network(input_dim, best_config)\n",
        "\n",
        "# Define inputs\n",
        "anchor_input = Input(shape=(input_dim,), name='anchor')\n",
        "positive_input = Input(shape=(input_dim,), name='positive')\n",
        "negative_input = Input(shape=(input_dim,), name='negative')\n",
        "\n",
        "# Process each input through the shared network\n",
        "anchor_output = shared_network(anchor_input)\n",
        "positive_output = shared_network(positive_input)\n",
        "negative_output = shared_network(negative_input)\n",
        "\n",
        "# Compute the distances\n",
        "distances = DistanceLayer()([anchor_output, positive_output, negative_output])\n",
        "\n",
        "# Define the final model\n",
        "final_siamese_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)\n",
        "\n",
        "# Display the model summary\n",
        "final_siamese_model.summary()\n",
        "\n",
        "# Compile the final model\n",
        "final_siamese_model.compile(optimizer='adam', loss=triplet_loss)\n",
        "\n",
        "# Prepare the data\n",
        "dummy_labels = np.zeros((train_anchors.shape[0], 1))\n",
        "\n",
        "# Train the final model\n",
        "final_siamese_model.fit([train_anchors, train_positives, train_negatives],\n",
        "                        dummy_labels,\n",
        "                        epochs=5,\n",
        "                        batch_size=32,\n",
        "                        validation_data=([dev_test_anchors, dev_test_positives, dev_test_negatives], np.zeros((dev_test_anchors.shape[0], 1))),\n",
        "                        verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enIK8EoSWUgM",
        "outputId": "b8781578-2ba2-409b-ec9e-e8e54f0d566b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final model with best hidden layer configuration: [128, 64, 32]\n",
            "Model: \"model_53\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " anchor (InputLayer)         [(None, 4071)]               0         []                            \n",
            "                                                                                                  \n",
            " positive (InputLayer)       [(None, 4071)]               0         []                            \n",
            "                                                                                                  \n",
            " negative (InputLayer)       [(None, 4071)]               0         []                            \n",
            "                                                                                                  \n",
            " model_52 (Functional)       (None, 32)                   531552    ['anchor[0][0]',              \n",
            "                                                                     'positive[0][0]',            \n",
            "                                                                     'negative[0][0]']            \n",
            "                                                                                                  \n",
            " distance_layer_26 (Distanc  (None, 2)                    0         ['model_52[0][0]',            \n",
            " eLayer)                                                             'model_52[1][0]',            \n",
            "                                                                     'model_52[2][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 531552 (2.03 MB)\n",
            "Trainable params: 531552 (2.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 3s 23ms/step - loss: 0.5063 - val_loss: 0.9068\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0068 - val_loss: 0.9296\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0030 - val_loss: 0.9360\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0025 - val_loss: 0.9316\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0026 - val_loss: 0.9345\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e01d8321ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "    \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
        "       The input questionids is a list of question ids.\n",
        "       The output is a list of lists of sentence ids\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(csvfile, nrows=300)\n",
        "    triplets = prepare_triplets(data)\n",
        "    qids, pos_ids, neg_ids, anchors, positives, negatives = vectorize_triplets(triplets, vectorizer)\n",
        "\n",
        "    summaries = []\n",
        "    for question_id in questionids:\n",
        "        question_triplets = [(triplet[0], triplet[2], triplet[4]) for triplet in triplets if triplet[0] == question_id]\n",
        "\n",
        "        if not question_triplets:\n",
        "            print(f\"No triplets found for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        question_anchors = np.array([anchors[i] for i, triplet in enumerate(triplets) if triplet[0] == question_id])\n",
        "        question_positives = np.array([positives[i] for i, triplet in enumerate(triplets) if triplet[0] == question_id])\n",
        "        question_negatives = np.array([negatives[i] for i, triplet in enumerate(triplets) if triplet[0] == question_id])\n",
        "\n",
        "        if question_anchors.size == 0 or question_positives.size == 0 or question_negatives.size == 0:\n",
        "            print(f\"Empty data encountered for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        scores = final_siamese_model.predict([question_anchors, question_positives, question_negatives])\n",
        "        ranked_indices = np.argsort(scores[:, 0])[:n]  # Since distances are returned in a tuple\n",
        "        ranked_sentids = [question_triplets[i][1] for i in ranked_indices]  # Get the pos_id for top-ranked sentences\n",
        "        summaries.append(ranked_sentids)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Test the summarizer function\n",
        "questionids = [6, 7, 10]\n",
        "top_n_sentences = nn_summariser(\"test.csv\", questionids, n=1)\n",
        "print(top_n_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaX3ohE3T6OF",
        "outputId": "f70c8467-44f3-41f8-f4db-a099ff168796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 5ms/step\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "[[19], [15], [10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_f1_score(csvfile, predicted_summaries, questionids):\n",
        "    data = pd.read_csv(csvfile)\n",
        "    true_labels = []\n",
        "\n",
        "    for qid in questionids:\n",
        "        question_data = data[data['qid'] == qid]\n",
        "        true_sentids = question_data[question_data['label'] == 1]['sentid'].tolist()\n",
        "        true_labels.append(true_sentids)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_summaries):\n",
        "        y_true.extend([1 if sid in true else 0 for sid in range(len(data))])\n",
        "        y_pred.extend([1 if sid in pred else 0 for sid in range(len(data))])\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Calculate F1 score for test set\n",
        "test_csv_path = 'test.csv'\n",
        "test_question_ids = test_data['qid'].unique().tolist()\n",
        "predicted_summaries = nn_summariser(test_csv_path, test_question_ids, n=1)\n",
        "precision, recall, f1 = calculate_f1_score(test_csv_path, predicted_summaries, test_question_ids)\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olv3EZjGT6mP",
        "outputId": "c6334e11-996e-4d0f-ec86-0c48ec9a2214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 8ms/step\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "3/3 [==============================] - 0s 8ms/step\n",
            "5/5 [==============================] - 0s 10ms/step\n",
            "5/5 [==============================] - 0s 9ms/step\n",
            "3/3 [==============================] - 0s 17ms/step\n",
            "8/8 [==============================] - 0s 14ms/step\n",
            "No triplets found for question ID 67\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "Precision: 0.8181818181818182, Recall: 0.16981132075471697, F1 Score: 0.28125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0NeK3gM0dC9"
      },
      "source": [
        "# Task 2 (5 marks): Recurrent NN\n",
        "\n",
        "Implement a more complex Siamese neural network that is composed of the following layers:\n",
        "\n",
        "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
        "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
        "* 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
        "\n",
        "Train the model with the training data, use the `dev_test` set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
        "\n",
        "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
        "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer (and length limit) and hidden layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenizer and data preparation from Task 1 are available\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "all_sentences = list(train_data['sentence text']) + list(dev_test_data['sentence text']) + list(test_data['sentence text'])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_sentences)\n",
        "\n",
        "# Analyze the distribution of sentence lengths\n",
        "all_sentences_lengths = [len(seq) for seq in tokenizer.texts_to_sequences(all_sentences)]\n",
        "plt.hist(all_sentences_lengths, bins=50)\n",
        "plt.xlabel('Length of sentences')\n",
        "plt.ylabel('Number of sentences')\n",
        "plt.show()\n",
        "\n",
        "# Determine the maximum length for 95% of the sentences\n",
        "max_len = int(np.percentile(all_sentences_lengths, 95))\n",
        "print(f\"Determined max length for 95% of sentences: {max_len}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "uxFCLNK3AMDn",
        "outputId": "824961ae-210b-4407-b8af-fca0c47fee89"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determined max length for 95% of sentences: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_siamese_rnn(input_dim, embedding_dim, lstm_size, hidden_layer_sizes, max_len):\n",
        "    input = Input(shape=(max_len,))\n",
        "    x = Embedding(input_dim, embedding_dim, input_length=max_len)(input)\n",
        "    x = LSTM(lstm_size)(x)\n",
        "    for size in hidden_layer_sizes:\n",
        "        x = Dense(size, activation='relu')(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "class DistanceLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor, positive, negative = inputs\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), axis=1, keepdims=True)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), axis=1, keepdims=True)\n",
        "        return tf.concat([ap_distance, an_distance], axis=1)\n",
        "\n",
        "def triplet_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    ap_distance = y_pred[:, 0]\n",
        "    an_distance = y_pred[:, 1]\n",
        "    return tf.reduce_mean(tf.maximum(ap_distance - an_distance + margin, 0))\n",
        "\n",
        "hidden_layer_configs = [\n",
        "    [128, 64, 32],\n",
        "    [256, 128, 64],\n",
        "    [64, 32, 16],\n",
        "    [128, 128, 128],\n",
        "]\n",
        "lstm_sizes = [64, 128, 256]\n",
        "\n",
        "best_config = None\n",
        "best_loss = float('inf')\n",
        "best_lstm_size = None\n"
      ],
      "metadata": {
        "id": "RrEMzsaOAMn6"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "def tokenize_triplets(triplets, tokenizer, max_len):\n",
        "    anchors, positives, negatives = [], [], []\n",
        "    for triplet in triplets:\n",
        "        anchor_seq = tokenizer.texts_to_sequences([triplet[1]])[0]\n",
        "        positive_seq = tokenizer.texts_to_sequences([triplet[3]])[0]\n",
        "        negative_seq = tokenizer.texts_to_sequences([triplet[5]])[0]\n",
        "        anchors.append(anchor_seq)\n",
        "        positives.append(positive_seq)\n",
        "        negatives.append(negative_seq)\n",
        "    return (pad_sequences(anchors, maxlen=max_len),\n",
        "            pad_sequences(positives, maxlen=max_len),\n",
        "            pad_sequences(negatives, maxlen=max_len))\n",
        "\n",
        "train_anchor, train_positive, train_negative = tokenize_triplets(train_triplets, tokenizer, max_len)\n",
        "dev_test_anchor, dev_test_positive, dev_test_negative = tokenize_triplets(dev_test_triplets, tokenizer, max_len)\n",
        "test_anchor, test_positive, test_negative = tokenize_triplets(test_triplets, tokenizer, max_len)\n"
      ],
      "metadata": {
        "id": "_1qz8i_KCWBD"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 35\n",
        "\n",
        "# Hyperparameter tuning\n",
        "for lstm_size in lstm_sizes:\n",
        "    for config in hidden_layer_configs:\n",
        "        print(f\"Evaluating model with LSTM size: {lstm_size} and hidden layer sizes: {config}\")\n",
        "\n",
        "        # Create the base network\n",
        "        shared_network = create_siamese_rnn(input_dim, embedding_dim, lstm_size, config, max_len)\n",
        "\n",
        "        # Define inputs\n",
        "        anchor_input = Input(shape=(max_len,), name='anchor')\n",
        "        positive_input = Input(shape=(max_len,), name='positive')\n",
        "        negative_input = Input(shape=(max_len,), name='negative')\n",
        "\n",
        "        # Process each input through the shared network\n",
        "        anchor_output = shared_network(anchor_input)\n",
        "        positive_output = shared_network(positive_input)\n",
        "        negative_output = shared_network(negative_input)\n",
        "\n",
        "        # Compute the distances\n",
        "        distances = DistanceLayer()([anchor_output, positive_output, negative_output])\n",
        "\n",
        "        # Define the model\n",
        "        siamese_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)\n",
        "\n",
        "        # Compile the model\n",
        "        siamese_model.compile(optimizer='adam', loss=triplet_loss)\n",
        "\n",
        "        # Train the model\n",
        "        dummy_labels = np.zeros((train_anchor.shape[0], 1))\n",
        "        siamese_model.fit([train_anchor, train_positive, train_negative],\n",
        "                          dummy_labels,\n",
        "                          epochs=5,\n",
        "                          batch_size=32,\n",
        "                          validation_data=([dev_test_anchor, dev_test_positive, dev_test_negative], np.zeros((dev_test_anchor.shape[0], 1))),\n",
        "                          verbose=1)\n",
        "\n",
        "        # Evaluate the model\n",
        "        loss = siamese_model.evaluate([dev_test_anchor, dev_test_positive, dev_test_negative], np.zeros((dev_test_anchor.shape[0], 1)), verbose=0)\n",
        "        print(f\"Validation loss: {loss}\")\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_config = config\n",
        "            best_lstm_size = lstm_size\n",
        "\n",
        "print(f\"Best LSTM size: {best_lstm_size} with hidden layer configuration: {best_config} and loss: {best_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EInx7SaeAM_l",
        "outputId": "cec61474-47c6-48fc-aa21-ed277abaaa67"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 14s 115ms/step - loss: 0.5724 - val_loss: 1.9355\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 6s 102ms/step - loss: 0.0243 - val_loss: 3.3612\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 5s 91ms/step - loss: 0.0095 - val_loss: 5.3016\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0122 - val_loss: 3.0603\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 10s 184ms/step - loss: 0.0055 - val_loss: 4.2927\n",
            "Validation loss: 4.29271936416626\n",
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 26s 105ms/step - loss: 0.5743 - val_loss: 1.7621\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 6s 110ms/step - loss: 0.0325 - val_loss: 3.3398\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 9s 160ms/step - loss: 0.0100 - val_loss: 1.9453\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 12s 211ms/step - loss: 0.0019 - val_loss: 1.8778\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 13s 226ms/step - loss: 0.0011 - val_loss: 2.0058\n",
            "Validation loss: 2.005756378173828\n",
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 17s 146ms/step - loss: 0.6398 - val_loss: 1.2404\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0504 - val_loss: 5.4343\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 6s 116ms/step - loss: 0.0120 - val_loss: 2.9197\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 5s 83ms/step - loss: 0.0172 - val_loss: 3.1498\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0062 - val_loss: 2.7979\n",
            "Validation loss: 2.7978596687316895\n",
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 15s 146ms/step - loss: 0.4927 - val_loss: 2.4856\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 6s 99ms/step - loss: 0.0400 - val_loss: 2.3155\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 5s 83ms/step - loss: 0.0078 - val_loss: 3.4690\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 7s 119ms/step - loss: 0.0037 - val_loss: 3.7814\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 4s 77ms/step - loss: 0.0081 - val_loss: 3.3919\n",
            "Validation loss: 3.39190411567688\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 22s 247ms/step - loss: 0.5489 - val_loss: 1.3649\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 12s 216ms/step - loss: 0.0288 - val_loss: 1.5908\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 13s 224ms/step - loss: 0.0081 - val_loss: 2.3837\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 14s 243ms/step - loss: 0.0095 - val_loss: 1.6717\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 12s 223ms/step - loss: 0.0126 - val_loss: 1.4105\n",
            "Validation loss: 1.4105067253112793\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 20s 254ms/step - loss: 0.5193 - val_loss: 1.3450\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 13s 235ms/step - loss: 0.0316 - val_loss: 2.5583\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 13s 242ms/step - loss: 0.0058 - val_loss: 5.8404\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 14s 245ms/step - loss: 0.0102 - val_loss: 2.0930\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 12s 221ms/step - loss: 0.0205 - val_loss: 3.8115\n",
            "Validation loss: 3.811502456665039\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 22s 250ms/step - loss: 0.6706 - val_loss: 1.0162\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 10s 186ms/step - loss: 0.0422 - val_loss: 1.7770\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 13s 233ms/step - loss: 0.0057 - val_loss: 2.2650\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 13s 235ms/step - loss: 0.0047 - val_loss: 2.5161\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 14s 245ms/step - loss: 0.0023 - val_loss: 3.3015\n",
            "Validation loss: 3.3015363216400146\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 20s 253ms/step - loss: 0.4667 - val_loss: 1.7119\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 13s 227ms/step - loss: 0.0296 - val_loss: 1.5256\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 14s 246ms/step - loss: 0.0090 - val_loss: 5.1468\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 11s 204ms/step - loss: 0.0127 - val_loss: 1.9489\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 11s 204ms/step - loss: 0.0031 - val_loss: 3.8368\n",
            "Validation loss: 3.8367841243743896\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 39s 578ms/step - loss: 0.5854 - val_loss: 1.7443\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 32s 566ms/step - loss: 0.0317 - val_loss: 2.2913\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 29s 513ms/step - loss: 0.0107 - val_loss: 3.3270\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 27s 482ms/step - loss: 0.0063 - val_loss: 4.0080\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 29s 524ms/step - loss: 0.0061 - val_loss: 3.1066\n",
            "Validation loss: 3.106630563735962\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 35s 525ms/step - loss: 0.4716 - val_loss: 1.4989\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 29s 524ms/step - loss: 0.0548 - val_loss: 1.3037\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 32s 577ms/step - loss: 0.0389 - val_loss: 3.7718\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 32s 571ms/step - loss: 0.0098 - val_loss: 7.2309\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 32s 574ms/step - loss: 0.0091 - val_loss: 8.8301\n",
            "Validation loss: 8.830106735229492\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 40s 580ms/step - loss: 0.5072 - val_loss: 2.1077\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 30s 539ms/step - loss: 0.0292 - val_loss: 4.9863\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.0163 - val_loss: 3.5565\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 31s 555ms/step - loss: 0.0112 - val_loss: 3.2594\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 0.0039 - val_loss: 2.9288\n",
            "Validation loss: 2.9287967681884766\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 38s 559ms/step - loss: 0.4736 - val_loss: 1.8983\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 0.0499 - val_loss: 2.1068\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 30s 543ms/step - loss: 0.0258 - val_loss: 1.5918\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 30s 539ms/step - loss: 0.0228 - val_loss: 1.0441\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 30s 536ms/step - loss: 0.0072 - val_loss: 2.5688\n",
            "Validation loss: 2.568826913833618\n",
            "Best LSTM size: 128 with hidden layer configuration: [128, 64, 32] and loss: 1.4105067253112793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training the final model with LSTM size: {best_lstm_size} and hidden layer configuration: {best_config}\")\n",
        "\n",
        "# Create the base network with the best configuration\n",
        "shared_network = create_siamese_rnn(input_dim, embedding_dim, best_lstm_size, best_config, max_len)\n",
        "\n",
        "# Define inputs\n",
        "anchor_input = Input(shape=(max_len,), name='anchor')\n",
        "positive_input = Input(shape=(max_len,), name='positive')\n",
        "negative_input = Input(shape=(max_len,), name='negative')\n",
        "\n",
        "# Process each input through the shared network\n",
        "anchor_output = shared_network(anchor_input)\n",
        "positive_output = shared_network(positive_input)\n",
        "negative_output = shared_network(negative_input)\n",
        "\n",
        "# Compute the distances\n",
        "distances = DistanceLayer()([anchor_output, positive_output, negative_output])\n",
        "\n",
        "# Define the final model\n",
        "final_siamese_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=distances)\n",
        "\n",
        "# Display the model summary\n",
        "final_siamese_model.summary()\n",
        "\n",
        "# Compile the final model\n",
        "final_siamese_model.compile(optimizer='adam', loss=triplet_loss)\n",
        "\n",
        "# Train the final model\n",
        "dummy_labels = np.zeros((train_anchor.shape[0], 1))\n",
        "final_siamese_model.fit([train_anchor, train_positive, train_negative],\n",
        "                        dummy_labels,\n",
        "                        epochs=5,\n",
        "                        batch_size=32,\n",
        "                        validation_data=([dev_test_anchor, dev_test_positive, dev_test_negative], np.zeros((dev_test_anchor.shape[0], 1))),\n",
        "                        verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBkxuXTpANHP",
        "outputId": "58a23305-0f48-41aa-e258-0ff9c95be8d1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final model with LSTM size: 128 and hidden layer configuration: [128, 64, 32]\n",
            "Model: \"model_79\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " anchor (InputLayer)         [(None, 43)]                 0         []                            \n",
            "                                                                                                  \n",
            " positive (InputLayer)       [(None, 43)]                 0         []                            \n",
            "                                                                                                  \n",
            " negative (InputLayer)       [(None, 43)]                 0         []                            \n",
            "                                                                                                  \n",
            " model_78 (Functional)       (None, 32)                   255296    ['anchor[0][0]',              \n",
            "                                                                     'positive[0][0]',            \n",
            "                                                                     'negative[0][0]']            \n",
            "                                                                                                  \n",
            " distance_layer_39 (Distanc  (None, 2)                    0         ['model_78[0][0]',            \n",
            " eLayer)                                                             'model_78[1][0]',            \n",
            "                                                                     'model_78[2][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 255296 (997.25 KB)\n",
            "Trainable params: 255296 (997.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 21s 274ms/step - loss: 0.6319 - val_loss: 1.8277\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 13s 227ms/step - loss: 0.0666 - val_loss: 1.5072\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 13s 225ms/step - loss: 0.0103 - val_loss: 2.7587\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 14s 243ms/step - loss: 0.0089 - val_loss: 2.4439\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 13s 234ms/step - loss: 0.0131 - val_loss: 2.0996\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e01e90bde70>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "    \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
        "       The input questionids is a list of question ids.\n",
        "       The output is a list of lists of sentence ids\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(csvfile, nrows=300)\n",
        "    triplets = prepare_triplets(data)\n",
        "    anchors, positives, negatives = tokenize_triplets(triplets, tokenizer, max_len)\n",
        "\n",
        "    summaries = []\n",
        "    for question_id in questionids:\n",
        "        question_triplets = [(triplet[0], triplet[2], triplet[4]) for triplet in triplets if triplet[0] == question_id]\n",
        "\n",
        "        if not question_triplets:\n",
        "            print(f\"No triplets found for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        question_anchors = np.array([anchors[i] for i, triplet in enumerate(triplets) if triplet[0] == question_id])\n",
        "        question_positives = np.array([positives[i] for i, triplet in enumerate(triplets) if triplet[0] == question_id])\n",
        "        question_negatives = np.array([negatives[i] for i, triplet in enumerate(triplets) if triplet[0] == question_id])\n",
        "\n",
        "        if question_anchors.size == 0 or question_positives.size == 0 or question_negatives.size == 0:\n",
        "            print(f\"Empty data encountered for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        scores = final_siamese_model.predict([question_anchors, question_positives, question_negatives])\n",
        "        ranked_indices = np.argsort(scores[:, 0])[:n]  # Since distances are returned in a tuple\n",
        "        ranked_sentids = [question_triplets[i][1] for i in ranked_indices]  # Get the pos_id for top-ranked sentences\n",
        "        summaries.append(ranked_sentids)\n",
        "\n",
        "    return summaries\n"
      ],
      "metadata": {
        "id": "6rtO6yDkANO2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test question IDs\n",
        "test_question_ids = [6, 7, 10]\n",
        "\n",
        "# Test the summarizer function\n",
        "top_n_sentences = nn_summariser('test.csv', test_question_ids, n=1)\n",
        "print(top_n_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYU0rnJZJYZd",
        "outputId": "4f376331-f46e-4ad1-a793-a26f749f4278"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 3s 73ms/step\n",
            "2/2 [==============================] - 0s 75ms/step\n",
            "4/4 [==============================] - 0s 73ms/step\n",
            "[[19], [0], [22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_score(csvfile, predicted_summaries, questionids):\n",
        "    data = pd.read_csv(csvfile)\n",
        "    true_labels = []\n",
        "\n",
        "    for qid in questionids:\n",
        "        question_data = data[data['qid'] == qid]\n",
        "        true_sentids = question_data[question_data['label'] == 1]['sentid'].tolist()\n",
        "        true_labels.append(true_sentids)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_summaries):\n",
        "        y_true.extend([1 if sid in true else 0 for sid in range(len(data))])\n",
        "        y_pred.extend([1 if sid in pred else 0 for sid in range(len(data))])\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Calculate F1 score for test set\n",
        "test_csv_path = 'test.csv'\n",
        "test_question_ids = test_data['qid'].unique().tolist()\n",
        "predicted_summaries = nn_summariser(test_csv_path, test_question_ids, n=1)\n",
        "precision, recall, f1 = calculate_f1_score(test_csv_path, predicted_summaries, test_question_ids)\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ova5HmxANVn",
        "outputId": "31a6a9b5-3ba7-4f2e-c7ba-af418e1c5cf1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 120ms/step\n",
            "2/2 [==============================] - 0s 66ms/step\n",
            "4/4 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "3/3 [==============================] - 0s 61ms/step\n",
            "5/5 [==============================] - 0s 80ms/step\n",
            "5/5 [==============================] - 0s 52ms/step\n",
            "3/3 [==============================] - 0s 51ms/step\n",
            "8/8 [==============================] - 0s 51ms/step\n",
            "No triplets found for question ID 67\n",
            "4/4 [==============================] - 0s 48ms/step\n",
            "4/4 [==============================] - 0s 45ms/step\n",
            "Precision: 0.9090909090909091, Recall: 0.18867924528301888, F1 Score: 0.3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison and Analysis**:\n",
        "\n",
        "**1.Precision:**\n",
        "\n",
        "The precision of the Task 2 system (0.909) is higher than that of the Task 1 system (0.818). This indicates that the Task 2 system has a higher proportion of correctly identified positive instances among all instances predicted as positive.\n",
        "\n",
        "**2.Recall:**\n",
        "\n",
        "The recall of the Task 2 system (0.189) is slightly higher than that of the Task 1 system (0.170). This indicates that the Task 2 system is marginally better at identifying the actual positive instances among all positive instances in the dataset.\n",
        "\n",
        "**3.F1 Score:**\n",
        "\n",
        "The F1 score, which is the harmonic mean of precision and recall, is higher for the Task 2 system (0.313) compared to the Task 1 system (0.281). The F1 score provides a balance between precision and recall, suggesting that the Task 2 system performs better overall in terms of both precision and recall.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The system developed in Task 2 outperforms the system developed in Task 1 in all evaluated metrics: precision, recall, and F1 score. The improvements in these metrics suggest that the more complex Siamese neural network with embedding, LSTM, and dense layers in Task 2 is better suited for the task of determining sentence similarity in this context."
      ],
      "metadata": {
        "id": "j6r3ydgtKxHC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiCobwJ2OxT3"
      },
      "source": [
        "# Task 3 (5 marks): Transformer\n",
        "\n",
        "Implement a simple Transformer neural network that is composed of the following layers:\n",
        "\n",
        "* Use BERT as feature extractor for each token.\n",
        "* A few of transformer encoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
        "* A few of transformer decoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
        "* 1 hidden layer with size 512.\n",
        "* The final output layer with one cell for binary classification to predict whether two inputs are related or not.\n",
        "\n",
        "Note that each input for this model should be a concatenation of a positive pair (i.e. question + one answer) or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence.\n",
        "\n",
        "Train the model with the training data, use the dev_test set to determine a good size of the transformer layers, and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the transformer layers.\n",
        "\n",
        "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain how to handle length difference for a batch of data\n",
        "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the transformer layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Load data\n",
        "train_data = pd.read_csv('training.csv').head(10)\n",
        "dev_test_data = pd.read_csv('dev_test.csv').head(5)\n",
        "test_data = pd.read_csv('test.csv').head(5)\n",
        "\n",
        "# Prepare triplets function from Task 1\n",
        "def prepare_triplets(data):\n",
        "    triplets = []\n",
        "    for qid, group in data.groupby('qid'):\n",
        "        question = group['question'].values[0]\n",
        "        positive_answers = group[group['label'] == 1][['sentid', 'sentence text']].values.tolist()\n",
        "        negative_answers = group[group['label'] == 0][['sentid', 'sentence text']].values.tolist()\n",
        "        for pos in positive_answers:\n",
        "            for neg in negative_answers:\n",
        "                triplets.append((qid, question, pos[0], pos[1], neg[0], neg[1]))\n",
        "    return triplets\n",
        "\n",
        "train_triplets = prepare_triplets(train_data)\n",
        "dev_test_triplets = prepare_triplets(dev_test_data)\n",
        "test_triplets = prepare_triplets(test_data)\n"
      ],
      "metadata": {
        "id": "fiEb_NzkAfFh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine all sentences to fit the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_pairs(triplets, tokenizer):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "    for triplet in triplets:\n",
        "        question, pos_sentence, neg_sentence = triplet[1], triplet[3], triplet[5]\n",
        "        pos_input = tokenizer.encode_plus(question, pos_sentence, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='tf')\n",
        "        neg_input = tokenizer.encode_plus(question, neg_sentence, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='tf')\n",
        "        input_ids.append(pos_input['input_ids'])\n",
        "        attention_masks.append(pos_input['attention_mask'])\n",
        "        labels.append(1)\n",
        "        input_ids.append(neg_input['input_ids'])\n",
        "        attention_masks.append(neg_input['attention_mask'])\n",
        "        labels.append(0)\n",
        "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0), tf.constant(labels)\n",
        "\n",
        "train_input_ids, train_attention_masks, train_labels = encode_pairs(train_triplets, tokenizer)\n",
        "dev_test_input_ids, dev_test_attention_masks, dev_test_labels = encode_pairs(dev_test_triplets, tokenizer)\n",
        "test_input_ids, test_attention_masks, test_labels = encode_pairs(test_triplets, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzcKgCnC8n2U",
        "outputId": "19fb9078-cec9-46e3-9d9e-e455e5b7b7bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(hidden_dim)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.mha(x, x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(hidden_dim)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training):\n",
        "        attn1 = self.mha1(x, x)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1)\n",
        "\n",
        "        attn2 = self.mha2(out1, enc_output)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n"
      ],
      "metadata": {
        "id": "b7qNeAwM8n_k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, num_enc_layers, num_dec_layers, hidden_dim, num_heads, ff_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.encoder_layers = [TransformerEncoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_enc_layers)]\n",
        "        self.decoder_layers = [TransformerDecoderLayer(hidden_dim, num_heads, ff_dim) for _ in range(num_dec_layers)]\n",
        "        self.dense = Dense(512, activation='relu')\n",
        "        self.output_layer = Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "        x = bert_output\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            x = enc_layer(x, training)\n",
        "\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            x = dec_layer(x, x, training)\n",
        "\n",
        "        x = self.dense(x[:, 0, :])\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Pg35-b9jAtJs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Determine number of layers\n",
        "encoder_layer_options = [1, 2, 3]\n",
        "decoder_layer_options = [1, 2, 3]\n",
        "hidden_dim = 768\n",
        "num_heads = 2\n",
        "ff_dim = 64\n",
        "\n",
        "best_encoder_layers = None\n",
        "best_decoder_layers = None\n",
        "best_loss = float('inf')\n"
      ],
      "metadata": {
        "id": "s-qfzfEfAwff"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for num_enc_layers in encoder_layer_options:\n",
        "    for num_dec_layers in decoder_layer_options:\n",
        "        print(f\"Evaluating model with {num_enc_layers} encoder layers and {num_dec_layers} decoder layers\")\n",
        "\n",
        "        model = TransformerModel(num_enc_layers, num_dec_layers, hidden_dim, num_heads, ff_dim)\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Prepare data for training\n",
        "        train_inputs = {'input_ids': train_input_ids, 'attention_mask': train_attention_masks}\n",
        "        dev_test_inputs = {'input_ids': dev_test_input_ids, 'attention_mask': dev_test_attention_masks}\n",
        "\n",
        "        model.fit(train_inputs, train_labels, epochs=1, batch_size=4, validation_data=(dev_test_inputs, dev_test_labels), verbose=1)\n",
        "\n",
        "        loss, accuracy = model.evaluate(dev_test_inputs, dev_test_labels, verbose=0)\n",
        "        print(f\"Validation loss: {loss}, accuracy: {accuracy}\")\n",
        "\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_encoder_layers = num_enc_layers\n",
        "            best_decoder_layers = num_dec_layers\n",
        "\n",
        "print(f\"Best model: {best_encoder_layers} encoder layers, {best_decoder_layers} decoder layers with loss: {best_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU_X9Uik8oIi",
        "outputId": "a1399869-3ca1-4eea-e52d-d8c4fb1819ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with 1 encoder layers and 1 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 499s 35s/step - loss: 1.8152 - accuracy: 0.3750 - val_loss: 0.8291 - val_accuracy: 0.5000\n",
            "Validation loss: 0.829144299030304, accuracy: 0.5\n",
            "Evaluating model with 1 encoder layers and 2 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 513s 38s/step - loss: 3.1513 - accuracy: 0.3750 - val_loss: 0.8214 - val_accuracy: 0.5000\n",
            "Validation loss: 0.8213696479797363, accuracy: 0.5\n",
            "Evaluating model with 1 encoder layers and 3 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - ETA: 0s - loss: 1.9816 - accuracy: 0.6667 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7a3908065240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 530s 40s/step - loss: 1.9816 - accuracy: 0.6667 - val_loss: 2.6982 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7a3908065240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.6982123851776123, accuracy: 0.5\n",
            "Evaluating model with 2 encoder layers and 1 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 489s 37s/step - loss: 1.7849 - accuracy: 0.5000 - val_loss: 0.7018 - val_accuracy: 0.5000\n",
            "Validation loss: 0.7017934918403625, accuracy: 0.5\n",
            "Evaluating model with 2 encoder layers and 2 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 525s 39s/step - loss: 2.3456 - accuracy: 0.4167 - val_loss: 3.1978 - val_accuracy: 0.5000\n",
            "Validation loss: 3.1977710723876953, accuracy: 0.5\n",
            "Evaluating model with 2 encoder layers and 3 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 548s 41s/step - loss: 1.5595 - accuracy: 0.5625 - val_loss: 0.7501 - val_accuracy: 0.5000\n",
            "Validation loss: 0.7501463890075684, accuracy: 0.5\n",
            "Evaluating model with 3 encoder layers and 1 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_6/bert/pooler/dense/kernel:0', 'tf_bert_model_6/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 490s 36s/step - loss: 2.5486 - accuracy: 0.5208 - val_loss: 1.5119 - val_accuracy: 0.5000\n",
            "Validation loss: 1.5118688344955444, accuracy: 0.5\n",
            "Evaluating model with 3 encoder layers and 2 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_7/bert/pooler/dense/kernel:0', 'tf_bert_model_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 531s 40s/step - loss: 1.7762 - accuracy: 0.6250 - val_loss: 1.4654 - val_accuracy: 0.5000\n",
            "Validation loss: 1.4653778076171875, accuracy: 0.5\n",
            "Evaluating model with 3 encoder layers and 3 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_8/bert/pooler/dense/kernel:0', 'tf_bert_model_8/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 570s 43s/step - loss: 1.6800 - accuracy: 0.5833 - val_loss: 0.9931 - val_accuracy: 0.5000\n",
            "Validation loss: 0.9930722117424011, accuracy: 0.5\n",
            "Best model: 2 encoder layers, 1 decoder layers with loss: 0.7017934918403625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_model = TransformerModel(best_encoder_layers, best_decoder_layers, hidden_dim, num_heads, ff_dim)\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "final_model.fit(train_inputs, train_labels, epochs=1, batch_size=4, validation_data=(dev_test_inputs, dev_test_labels), verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VycjzyF48oR1",
        "outputId": "974a52af-10c6-4e7b-a8ff-4a98b8d94911"
      },
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 471s 35s/step - loss: 2.1128 - accuracy: 0.5417 - val_loss: 0.7707 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a39080d97b0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "    data = pd.read_csv(csvfile, nrows=10)\n",
        "    triplets = prepare_triplets(data)\n",
        "    input_ids, attention_masks, labels = encode_pairs(triplets, tokenizer)\n",
        "\n",
        "    summaries = []\n",
        "    for question_id in questionids:\n",
        "        question_indices = [i for i, triplet in enumerate(triplets) if triplet[0] == question_id]\n",
        "\n",
        "        if not question_indices:\n",
        "            print(f\"No pairs found for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        question_inputs = {\n",
        "            'input_ids': tf.gather(input_ids, question_indices),\n",
        "            'attention_mask': tf.gather(attention_masks, question_indices)\n",
        "        }\n",
        "\n",
        "        scores = final_model.predict(question_inputs)\n",
        "        ranked_indices = np.argsort(scores[:, 0])[:n]\n",
        "        ranked_sentids = [triplets[question_indices[i]][2] for i in ranked_indices]  # Get the sentid for top-ranked sentences\n",
        "        summaries.append(ranked_sentids)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Test the summarizer function\n",
        "questionids = [6,7,10]\n",
        "top_n_sentences = nn_summariser(\"test.csv\", questionids, n=1)\n",
        "print(top_n_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHB6Zesq8obg",
        "outputId": "92b32f65-cbb4-4efb-a9bb-b5d88c5674f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 21s 21s/step\n",
            "No pairs found for question ID 7\n",
            "No pairs found for question ID 10\n",
            "[[0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_f1_score(csvfile, predicted_summaries, questionids):\n",
        "    data = pd.read_csv(csvfile)\n",
        "    true_labels = []\n",
        "\n",
        "    for qid in questionids:\n",
        "        question_data = data[data['qid'] == qid]\n",
        "        true_sentids = question_data[question_data['label'] == 1]['sentid'].tolist()\n",
        "        true_labels.append(true_sentids)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_summaries):\n",
        "        y_true.extend([1 if sid in true else 0 for sid in range(len(data))])\n",
        "        y_pred.extend([1 if sid in pred else 0 for sid in range(len(data))])\n",
        "\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Calculate F1 score for test set\n",
        "test_csv_path = 'test.csv'\n",
        "test_question_ids = test_data['qid'].unique().tolist()\n",
        "predicted_summaries = nn_summariser(test_csv_path, test_question_ids, n=1)\n",
        "precision, recall, f1 = calculate_f1_score(test_csv_path, predicted_summaries, test_question_ids)\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhHfkIEc8okG",
        "outputId": "5be998ca-8a8c-46b7-9847-3f4d5c5db89e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 23s 23s/step\n",
            "Precision: 1.0, Recall: 0.2, F1 Score: 0.33333333333333337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison and Analysis:**\n",
        "\n",
        "**Precision:**\n",
        " In Task 2, the precision is 0.9091, which means about 91% of the predicted positive instances are actually positive. In Task 3, the precision is 1.0, indicating that all predicted positive instances are indeed positive. Therefore, Task 3 has a perfect precision score, which is better than Task 2.\n",
        "\n",
        "**Recall:**\n",
        "  Task 2 has a recall of 0.1887, meaning it identifies about 19% of all actual positives. Task 3 has a slightly higher recall of 0.2, identifying 20% of the actual positives. Although the difference is small, Task 3 has a better recall.\n",
        "\n",
        "**F1 Score:**\n",
        " Task 2 has an F1 score of 0.3125, while Task 3 has a slightly higher F1 score of 0.3333. Since the F1 score is a better overall measure of model performance when there is an imbalance between precision and recall, Task 3 is better in this regard.\n",
        "\n",
        "**Conclusion:**\n",
        "Based on the precision, recall, and F1 score metrics, the system developed in Task 3 is better than the system developed in Task 2. Task 3 has a higher precision, recall, and F1 score, indicating it performs better in accurately predicting positive instances, identifying actual positives, and balancing precision and recall. Therefore, Task 3's model shows a slight improvement in overall performance compared to Task 2's model."
      ],
      "metadata": {
        "id": "ixXZpLzAoCPO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppkBsuB_0dC9"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
        "\n",
        "**Do not submit multiple files.**\n",
        "\n",
        "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}